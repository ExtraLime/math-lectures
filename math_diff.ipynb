{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Lectures\n",
    "### Unit 4 Capstone (Thinkful Data Science)\n",
    "This notebook examines math lectures on the sentence level. The lectures are vectorized using tf-idf and topics are extracted with Non negative matrix factorization and Latent Dirichlet Allocation.  Here we try to extract topics from AI, Ai10 lecture and Diff eq, Data Structures and Algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import islice\n",
    "import smart_open\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import xml.etree.ElementTree as ET  \n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn import cluster, ensemble, linear_model, naive_bayes, neighbors, svm\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, ShuffleSplit\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA, LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer, minmax_scale\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc, silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    module='gensim',\n",
    "    action='ignore',\n",
    "    message='Conversion')\n",
    "warnings.filterwarnings(\n",
    "    module='scipy',\n",
    "    action='ignore',\n",
    "    message='Using a')\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "\n",
    "default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "sns.set(style='dark',context='paper',palette='BrBG_r')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    " \n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #%d: \\n\" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_time1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data\n",
    "    - The texts consist of the closed captioning for 93 lectures on varying topics in mathematics, computer science, and engineering\n",
    "    - The XML files were accessed through the console in a web browser and saved as XML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path to XML files\n",
    "path = '/root/Desktop/gits/unit-4-NLP-capstone/lectures'\n",
    "folder = os.fsencode(path)\n",
    "\n",
    "#get list of the filenames\n",
    "filenames = sorted([re.sub('.xml','',os.fsdecode(file)) for file in os.listdir(folder)])\n",
    "\\\n",
    "\n",
    "#Use ElementTree trace xml tree and extract text from xml files, removing tags\n",
    "lecture_texts = []\n",
    "for file in filenames:\n",
    "    tree = ET.parse('lectures/{}.xml'.format(file))\n",
    "    root = tree.getroot()\n",
    "    all_text = []\n",
    "    for elem in root:  \n",
    "        for subelem in elem:\n",
    "            all_text.append(subelem.text)\n",
    "    lecture_texts.append(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the most common change of\\nvariables is the one simple one',\n",
       " 'called scaling.\\nSo, again, the kind of equation',\n",
       " \"I'm talking about is a general\\nfirst-order equation.\",\n",
       " 'And, scaling simply means to\\nchange the coordinates,',\n",
       " 'in effect, or axes,\\nto change the coordinates on']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check some text from a random lecture\n",
    "lecture_texts[47][30:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aurouxmcalc1</td>\n",
       "      <td>So let's start right away with\\nstuff that we ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename                                           raw_text\n",
       "0  aurouxmcalc1  So let's start right away with\\nstuff that we ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a DataFrame for records\n",
    "raw_data = pd.DataFrame()\n",
    "raw_data['filename'] = filenames\n",
    "raw_data['raw_text'] = [ ' '.join(i) for i in lecture_texts ] #unpack list of lists as string\n",
    "print(type(raw_data.raw_text[0]))\n",
    "raw_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will clean the text removing '\\n', the intros (MIT has a long intro), the professor names and other undesirable punctuation etc. Also, we want to replace contractions with thier full counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove identified words through visual inspection\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',', ',text) # unrecognized punctuation\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text) #remove in [audible],[laughter] etc\n",
    "    text = ' '.join(text.split()) #for one long string\n",
    "    #remove intros,professor names\n",
    "    text = re.sub(\"The following content is provided under a Creative Commons license. \",'',text)\n",
    "    text = re.sub('Your support will help MIT OpenCourseWare continue to',' ',text)\n",
    "    text = re.sub(' offer high quality educational resources for free.',' ',text)\n",
    "    text = re.sub('To make a donation or ',' ',text)\n",
    "    text = re.sub('view additional materials from hundreds of MIT',' ',text)   \n",
    "    text = re.sub(' courses, visit MIT OpenCourseWare at ocw.mit.edu.',' ',text)   \n",
    "    text = re.sub('Haynes Miller','PROFESSOR',text)\n",
    "    text = re.sub('David Jerison','PROFESSOR',text)\n",
    "    text = re.sub('Srini Devadas','PROFESSOR',text)\n",
    "    text = re.sub('Gilbert Strang','PROFESSOR',text)\n",
    "    text = re.sub('TOM LEIGHTON','PROFESSOR',text)\n",
    "    text = re.sub('PHILIPPE RIGOLLET','PROFESSOR',text)\n",
    "    text = re.sub('PROFESSOR STRANG','PROFESSOR',text)\n",
    "    text = re.sub('PROF. PATRICK WINSTON','PROFESSOR',text)\n",
    "    text = re.sub('PROFESSOR PATRICK WINSTON','PROFESSOR',text)   \n",
    "    text = re.sub('ERIK DEMAINE','PROFESSOR',text)\n",
    "    text = re.sub('Demaine','PROFESSOR',text)\n",
    "    text = re.sub('Erik','PROFESSOR',text)\n",
    "    text = re.sub('PROF. JERISON','PROFESSOR',text)\n",
    "    text = re.sub('ERIK DOMANE','PROFESSOR',text)\n",
    "    text = re.sub('Erik Domane','PROFESSOR',text)\n",
    "    text = re.sub('Stanford University. >>', '',text)\n",
    "    text = re.sub('PATRICK WINSTON', 'PROFESSOR',text)\n",
    "    text = re.sub('Welcome to 6.851 Advanced Data Structures', 'PROFESSOR',text)\n",
    "    text = re.sub('PROFESSOR: PROFESSOR', ' ',text)\n",
    "    text = re.sub('PROFESSOR PROFESSOR', ' ',text)\n",
    "    return text\n",
    "\n",
    "#list common contractions and full text counterpart\n",
    "contractions_dict = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"does'nt\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he has\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it would\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she had / she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"tellin\": 'telling',\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"thats\": \"that is\",\n",
    "  \"there'd\": \"there would\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we would\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"ya\": \"you\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had / you would\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you shall / you will\",\n",
    "  \"you'll've\": \"you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"}\n",
    "#function to correct the contractions\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.raw_text = [ expand_contractions(i) for i in raw_data.raw_text]\n",
    "raw_data.raw_text = [ text_cleaner(i) for i in raw_data.raw_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>Professor</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aurouxmcalc1</td>\n",
       "      <td>So let us start right away with stuff that we ...</td>\n",
       "      <td>Auroux</td>\n",
       "      <td>Calculus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aurouxmcalc11</td>\n",
       "      <td>to    So far we have learned about partial...</td>\n",
       "      <td>Auroux</td>\n",
       "      <td>Calculus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aurouxmcalc2</td>\n",
       "      <td>So , So, yesterday we learned about the questi...</td>\n",
       "      <td>Auroux</td>\n",
       "      <td>Calculus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aurouxmcalc5</td>\n",
       "      <td>to    So, so far, we have seen things abou...</td>\n",
       "      <td>Auroux</td>\n",
       "      <td>Calculus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>demainedata1</td>\n",
       "      <td>. I am  . You can call me PROFESSOR. W...</td>\n",
       "      <td>Demaine</td>\n",
       "      <td>Data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                           raw_text Professor  \\\n",
       "0   aurouxmcalc1  So let us start right away with stuff that we ...    Auroux   \n",
       "1  aurouxmcalc11      to    So far we have learned about partial...    Auroux   \n",
       "2   aurouxmcalc2  So , So, yesterday we learned about the questi...    Auroux   \n",
       "3   aurouxmcalc5      to    So, so far, we have seen things abou...    Auroux   \n",
       "4   demainedata1          . I am  . You can call me PROFESSOR. W...   Demaine   \n",
       "\n",
       "    Subject  \n",
       "0  Calculus  \n",
       "1  Calculus  \n",
       "2  Calculus  \n",
       "3  Calculus  \n",
       "4      Data  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label professors\n",
    "raw_data['Professor'] = ['Auroux']*4 + ['Demaine']*7 + ['Devadas']*9 + ['Jerison']*9 + ['Leighton']*11 + ['Manning']*3 + ['Mattuck']*9\\\n",
    "+ ['Rigollet']*10 + ['Socher']*4 + ['Strang']*18 + ['Winston']* 8\n",
    "#label_subjects\n",
    "raw_data['Subject'] = ['Calculus']*4 + ['Data']*7 + ['Algorithms']*9 + ['Calculus']*9 + ['CS Math']*11 + ['NLP']*3\\\n",
    "+ ['Diff. Eq.']*9+ ['Statistics']*10 + ['NLP']*4 + ['Linear Algebra']*10 + ['Mech. Eng']* 8 +\\\n",
    "['AI']*8\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will parse the text using spacy and append each spacy object (each lecture) to the dataframe.\n",
    "Then we will create a new data frame with the professor, subject, filename and the spacy doc.\n",
    "We will use a list comprehension to tokenize each sentences from each lecture and append the list of lists to the dataframe.\n",
    "Then we will drop the spacy doc, we will -explode- the data frame so that each sentence is an observation, while maintaining tracability to the lecture level.\n",
    "We will also take the sentence length for further sentence elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minutes: 4.693093649546305\n"
     ]
    }
   ],
   "source": [
    "#Create Spacy document for each lecture\n",
    "t1 = time.time()\n",
    "nlp = spacy.load('en')\n",
    "raw_data['sdoc'] = [nlp(i) for i in raw_data.raw_text]\n",
    "print('Minutes: {}'.format((time.time()-t1)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more cleaning\n",
    "def clean_lite (text):\n",
    "    re.sub(\"-pron-\",'',text)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new data frame for the professor,subject and the spacy doc\n",
    "sentences = raw_data[['filename','Professor','Subject','sdoc']].copy()\n",
    "\n",
    "#create a list of lists of tokens (remove stop words and punct) \n",
    "sentences['sents'] = [ [ [token.lemma_.lower() for token in sent if not token.is_stop\n",
    "        and not token.is_punct] for sent in doc.sents] for doc in sentences.sdoc]\n",
    "\n",
    "#convert lecture lists of sentences to lecture string\n",
    "sentences['text'] = [' '.join([str( ' '.join(i)) for i in j]) for j in sentences.sents]\n",
    "\n",
    "sentences['text'] = [ ' '.join(pd.Series(sentences.text[i]).str.replace('-pron-',''))\\\n",
    "                     for i in range(len(sentences.text)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Professor</th>\n",
       "      <th>Subject</th>\n",
       "      <th>sdoc</th>\n",
       "      <th>sents</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aurouxmcalc1</td>\n",
       "      <td>Auroux</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>(So, let, us, start, right, away, with, stuff,...</td>\n",
       "      <td>[[so, let, start, right, away, stuff, need, ad...</td>\n",
       "      <td>so let start right away stuff need advanced th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aurouxmcalc11</td>\n",
       "      <td>Auroux</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>(    , to,    , So, far, we, have, learned, ab...</td>\n",
       "      <td>[[    ,    ], [so, far, learn, partial, deriva...</td>\n",
       "      <td>so far learn partial derivative use f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aurouxmcalc2</td>\n",
       "      <td>Auroux</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>(So, ,, So, ,, yesterday, we, learned, about, ...</td>\n",
       "      <td>[[so, so, yesterday, learn, question, plane, t...</td>\n",
       "      <td>so so yesterday learn question plane think 3x3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aurouxmcalc5</td>\n",
       "      <td>Auroux</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>(    , to,    , So, ,, so, far, ,, we, have, s...</td>\n",
       "      <td>[[    ,    ], [so, far, see, thing, vector, eq...</td>\n",
       "      <td>so far see thing vector equation plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>demainedata1</td>\n",
       "      <td>Demaine</td>\n",
       "      <td>Data</td>\n",
       "      <td>(        , ., I, am,  , ., You, can, call, me,...</td>\n",
       "      <td>[[        ], [-pron-,  ], [-pron-, professor],...</td>\n",
       "      <td>professor  ta tom morgan justin z...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename Professor   Subject  \\\n",
       "0   aurouxmcalc1    Auroux  Calculus   \n",
       "1  aurouxmcalc11    Auroux  Calculus   \n",
       "2   aurouxmcalc2    Auroux  Calculus   \n",
       "3   aurouxmcalc5    Auroux  Calculus   \n",
       "4   demainedata1   Demaine      Data   \n",
       "\n",
       "                                                sdoc  \\\n",
       "0  (So, let, us, start, right, away, with, stuff,...   \n",
       "1  (    , to,    , So, far, we, have, learned, ab...   \n",
       "2  (So, ,, So, ,, yesterday, we, learned, about, ...   \n",
       "3  (    , to,    , So, ,, so, far, ,, we, have, s...   \n",
       "4  (        , ., I, am,  , ., You, can, call, me,...   \n",
       "\n",
       "                                               sents  \\\n",
       "0  [[so, let, start, right, away, stuff, need, ad...   \n",
       "1  [[    ,    ], [so, far, learn, partial, deriva...   \n",
       "2  [[so, so, yesterday, learn, question, plane, t...   \n",
       "3  [[    ,    ], [so, far, see, thing, vector, eq...   \n",
       "4  [[        ], [-pron-,  ], [-pron-, professor],...   \n",
       "\n",
       "                                                text  \n",
       "0  so let start right away stuff need advanced th...  \n",
       "1           so far learn partial derivative use f...  \n",
       "2  so so yesterday learn question plane think 3x3...  \n",
       "3           so far see thing vector equation plan...  \n",
       "4               professor  ta tom morgan justin z...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentences.to_csv('sentences.csv')\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking the lectures down by sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an attempt to find out why data structures and algorithms refuse to separate, why an AI lecture consistently jumps to Differential Equations. We will perform some LSA (latent semantic analysis) on the the groups in question, these groups include: Data Structures, Algorithims and the lecture from AI, compared to the rest of AI and compared to differential equations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explode the df to a new df with each sentence on one line\n",
    "sentences1 = pd.DataFrame({\n",
    "         col:np.repeat(sentences[col].values, sentences['sents'].str.len())\n",
    "         for col in sentences.columns.difference(['sents'])\n",
    "         }).assign(**{'sents':np.concatenate(sentences['sents'].values)})[sentences.columns.tolist()]\n",
    "\n",
    "#get sentence lengths\n",
    "sentences1['s_len'] = [len(sent) for sent in sentences1.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Professor</th>\n",
       "      <th>Subject</th>\n",
       "      <th>sdoc</th>\n",
       "      <th>sents</th>\n",
       "      <th>text</th>\n",
       "      <th>s_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59242</th>\n",
       "      <td>strangmeng5</td>\n",
       "      <td>Strang</td>\n",
       "      <td>Mech. Eng</td>\n",
       "      <td>(   , To, make, a, donation, ,, or, to,    , P...</td>\n",
       "      <td>[-pron-, be, get, zero, will]</td>\n",
       "      <td>to donation     professor shall start the ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11965</th>\n",
       "      <td>devadasalgos3</td>\n",
       "      <td>Devadas</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>(       , PROFESSOR, :, So, today, 's, lecture...</td>\n",
       "      <td>[the, reason, interested, algorithm, people, w...</td>\n",
       "      <td>professor so today 's lecture sort  wi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5335</th>\n",
       "      <td>demainedata15</td>\n",
       "      <td>Demaine</td>\n",
       "      <td>Data</td>\n",
       "      <td>(       , PROFESSOR, :, All, right, ., Today, ...</td>\n",
       "      <td>[but, general, small, power, 2]</td>\n",
       "      <td>professor all right today go look kind...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57511</th>\n",
       "      <td>strangmeng2</td>\n",
       "      <td>Strang</td>\n",
       "      <td>Mech. Eng</td>\n",
       "      <td>(Your, support, will, help, MIT, OpenCourseWar...</td>\n",
       "      <td>[so, be, differential, equation]</td>\n",
       "      <td>support help mit opencourseware        profes...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23516</th>\n",
       "      <td>leighton15linearcs</td>\n",
       "      <td>Leighton</td>\n",
       "      <td>CS Math</td>\n",
       "      <td>(  , offer, high, -, quality, educational, res...</td>\n",
       "      <td>[professor]</td>\n",
       "      <td>offer high quality educational resource fre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 filename Professor     Subject  \\\n",
       "59242         strangmeng5    Strang   Mech. Eng   \n",
       "11965       devadasalgos3   Devadas  Algorithms   \n",
       "5335        demainedata15   Demaine        Data   \n",
       "57511         strangmeng2    Strang   Mech. Eng   \n",
       "23516  leighton15linearcs  Leighton     CS Math   \n",
       "\n",
       "                                                    sdoc  \\\n",
       "59242  (   , To, make, a, donation, ,, or, to,    , P...   \n",
       "11965  (       , PROFESSOR, :, So, today, 's, lecture...   \n",
       "5335   (       , PROFESSOR, :, All, right, ., Today, ...   \n",
       "57511  (Your, support, will, help, MIT, OpenCourseWar...   \n",
       "23516  (  , offer, high, -, quality, educational, res...   \n",
       "\n",
       "                                                   sents  \\\n",
       "59242                      [-pron-, be, get, zero, will]   \n",
       "11965  [the, reason, interested, algorithm, people, w...   \n",
       "5335                     [but, general, small, power, 2]   \n",
       "57511                   [so, be, differential, equation]   \n",
       "23516                                        [professor]   \n",
       "\n",
       "                                                    text  s_len  \n",
       "59242      to donation     professor shall start the ...      5  \n",
       "11965          professor so today 's lecture sort  wi...      7  \n",
       "5335           professor all right today go look kind...      5  \n",
       "57511   support help mit opencourseware        profes...      4  \n",
       "23516     offer high quality educational resource fre...      1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to make sure distribution of attributes to sentences\n",
    "sentences1.sample(5,random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65569, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    65569.000000\n",
       "mean         5.470649\n",
       "std          3.800854\n",
       "min          0.000000\n",
       "25%          3.000000\n",
       "50%          5.000000\n",
       "75%          7.000000\n",
       "max         49.000000\n",
       "Name: s_len, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "sns.distplot(tuple(sentences1.s_len),bins=15)\n",
    "plt.title('Distribution of Sentence Length')\n",
    "plt.show()\n",
    "\n",
    "sentences1.s_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65569, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently we have 65000+ sentences that need to be vectorized and processed. In order to avoid complications with the vectorization and clustering, I will set a minimum word length for each sentence.\n",
    "    \n",
    "    - Create a list of index numbers to keep\n",
    "    - Replace the dataframe with only desired index numbers\n",
    "    - Convert lists of tokens to strings\n",
    "    - Remove '-pron-' token created by spacy\n",
    "We've set a window of 9 to 16 sentence size. This will reduce the number of sentences to around 10,000, while hopefully still preserving the substance of the texts.  This is really a compromise. Vectorizing and processing 66,000 sentences is computationally and time intensive, so we want the reduction. Plus, I don't want the model to think sentences like \"That's Right\" or \"Now, what?\" or any other short meaningless statement you would hear in a math lecture are more important than longer sentences describing a concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Professor</th>\n",
       "      <th>Subject</th>\n",
       "      <th>sdoc</th>\n",
       "      <th>sents</th>\n",
       "      <th>text</th>\n",
       "      <th>s_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3177</th>\n",
       "      <td>leighton14dandcs</td>\n",
       "      <td>Leighton</td>\n",
       "      <td>CS Math</td>\n",
       "      <td>(       , PROFESSOR, :, This, week, we, are, g...</td>\n",
       "      <td>[all, right, like, know, long, till, world, en...</td>\n",
       "      <td>all right like know long till world end t64</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641</th>\n",
       "      <td>rigolletstats19</td>\n",
       "      <td>Rigollet</td>\n",
       "      <td>Statistics</td>\n",
       "      <td>(    , to,    , PROFESSOR, :, ,, bunch, of, x,...</td>\n",
       "      <td>[think, realization, guy, go, cloud, n, point,...</td>\n",
       "      <td>think realization guy go cloud n point r d.</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9408</th>\n",
       "      <td>strangmeng6</td>\n",
       "      <td>Strang</td>\n",
       "      <td>Mech. Eng</td>\n",
       "      <td>(   , To, make, a, donation, ,, or, to,    , P...</td>\n",
       "      <td>[so, -pron-, want, sum, square, -pron-, think,...</td>\n",
       "      <td>so  want sum square  think cos(theta sin(theta</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7650</th>\n",
       "      <td>sochernlp13</td>\n",
       "      <td>Socher</td>\n",
       "      <td>NLP</td>\n",
       "      <td>( , Network, ,, there, is, actually, a, whole,...</td>\n",
       "      <td>[well, model, go, see, small, norm, feature, v...</td>\n",
       "      <td>well model go see small norm feature vector z ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>jerisoncalc21</td>\n",
       "      <td>Jerison</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>(   , To, make, a, donation, ,, or, to,    , P...</td>\n",
       "      <td>[and, -pron-, think, order, answer, question, ...</td>\n",
       "      <td>and  think order answer question  go detail me...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename Professor     Subject  \\\n",
       "3177  leighton14dandcs  Leighton     CS Math   \n",
       "6641   rigolletstats19  Rigollet  Statistics   \n",
       "9408       strangmeng6    Strang   Mech. Eng   \n",
       "7650       sochernlp13    Socher         NLP   \n",
       "2590     jerisoncalc21   Jerison    Calculus   \n",
       "\n",
       "                                                   sdoc  \\\n",
       "3177  (       , PROFESSOR, :, This, week, we, are, g...   \n",
       "6641  (    , to,    , PROFESSOR, :, ,, bunch, of, x,...   \n",
       "9408  (   , To, make, a, donation, ,, or, to,    , P...   \n",
       "7650  ( , Network, ,, there, is, actually, a, whole,...   \n",
       "2590  (   , To, make, a, donation, ,, or, to,    , P...   \n",
       "\n",
       "                                                  sents  \\\n",
       "3177  [all, right, like, know, long, till, world, en...   \n",
       "6641  [think, realization, guy, go, cloud, n, point,...   \n",
       "9408  [so, -pron-, want, sum, square, -pron-, think,...   \n",
       "7650  [well, model, go, see, small, norm, feature, v...   \n",
       "2590  [and, -pron-, think, order, answer, question, ...   \n",
       "\n",
       "                                                   text  s_len  \n",
       "3177        all right like know long till world end t64      9  \n",
       "6641        think realization guy go cloud n point r d.      9  \n",
       "9408     so  want sum square  think cos(theta sin(theta      9  \n",
       "7650  well model go see small norm feature vector z ...     11  \n",
       "2590  and  think order answer question  go detail me...     12  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list index numbers to keep\n",
    "to_keep = [ i for i in sentences1.index if len(sentences1.sents[i]) >= 9 and len(sentences1.sents[i]) <= 16]\n",
    "#keep index numbers,execute other steps\n",
    "sentences1 = sentences1.iloc[to_keep]\n",
    "sentences1['text'] = [' '.join(i) for i in list(sentences1.sents)]\n",
    "sentences1.text = [ re.sub(\"-pron-\",\"\",sentences1.text[i]) for i in to_keep]\n",
    "sentences1.reset_index(inplace=True, drop=True) #reset the index here\n",
    "sentences1.sample(5, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10156, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting topics from select subjects and lectures.\n",
    "    - We want to perform LSA on the following:\n",
    "        - All AI lectures, except for the lecture that switches to Differential equations.\n",
    "        - The AI lecture that switches to Differential Equations\n",
    "        - Differential Equations lectures as a subject\n",
    "        - Data Structures\n",
    "        - Algorithims "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Vectorization of the sentences\n",
    "    Right now we are going to create numerical vectors for each sentence of our given groups using the Tf-idf feature from the genism library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, # drop words that occur in more 50% of the sentences\n",
    "                             min_df=5, # only use words that appear at least 5 times\n",
    "                             stop_words='english', \n",
    "                             lowercase=True,\n",
    "                             use_idf=True,\n",
    "                             norm=u'l2',  \n",
    "                             smooth_idf=True \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate each group\n",
    "AI = sentences1[(sentences1.Subject == 'AI') & (sentences1.filename != 'winstonai10')]['text']\n",
    "winstonai10 = sentences1[sentences1.filename == 'winstonai10']['text']\n",
    "diff_eq = sentences1[sentences1.Subject == 'Diff. Eq.']['text']\n",
    "datas = sentences1[sentences1.Subject == 'Data']['text']\n",
    "algos = sentences1[sentences1.Subject == 'Algorithms']['text']\n",
    "mattuck4 = sentences1[sentences1.filename == 'mattuckdifeq4']['text']\n",
    "\n",
    "groups = [AI, winstonai10, diff_eq, datas, algos]\n",
    "\n",
    "#Vectorize Each grouping\n",
    "AI_tfidf  = vectorizer.fit_transform(AI)\n",
    "winston_tfidf  = vectorizer.fit_transform(winstonai10)\n",
    "diff_tfidf  = vectorizer.fit_transform(diff_eq)\n",
    "datas_tfidf  = vectorizer.fit_transform(datas)\n",
    "algos_tfidf  = vectorizer.fit_transform(algos)\n",
    "matt4_tfidf  = vectorizer.fit_transform(mattuck4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((572, 400), (91, 73), (767, 427), (1101, 512), (864, 464), (104, 72))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_tfidf.shape,winston_tfidf.shape, diff_tfidf.shape, datas_tfidf.shape, algos_tfidf.shape, matt4_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to calculate the cosine similarity of the sentences in each grouping, then calculate the mean similarity, this will hopefully give us an idea what representetive sentences might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_sim = pd.DataFrame(cosine_similarity(AI_tfidf), index=AI,\n",
    "                            columns=AI) # set sentneces as the columns\n",
    "ai_sim.insert(0, 'mean_similarity', ai_sim.mean(axis=1))\n",
    "\n",
    "\n",
    "win_sim = pd.DataFrame(cosine_similarity(winston_tfidf), index=winstonai10,\n",
    "                            columns=winstonai10) # set sentneces as the columns\n",
    "win_sim.insert(0, 'mean_similarity', win_sim.mean(axis=1))\n",
    "\n",
    "\n",
    "diff_sim = pd.DataFrame(cosine_similarity(diff_tfidf), index=diff_eq,\n",
    "                            columns=diff_eq) # set sentneces as the columns\n",
    "diff_sim.insert(0, 'mean_similarity', diff_sim.mean(axis=1))\n",
    "\n",
    "\n",
    "data_sim = pd.DataFrame(cosine_similarity(datas_tfidf), index=datas,\n",
    "                            columns=datas) # set sentneces as the columns\n",
    "data_sim.insert(0, 'mean_similarity', data_sim.mean(axis=1))\n",
    "\n",
    "\n",
    "\n",
    "algos_sim = pd.DataFrame(cosine_similarity(algos_tfidf), index=algos,\n",
    "                            columns=algos) # set sentneces as the columns\n",
    "algos_sim.insert(0, 'mean_similarity', algos_sim.mean(axis=1))\n",
    "\n",
    "\n",
    "matt_sim = pd.DataFrame(cosine_similarity(matt4_tfidf), index=mattuck4,\n",
    "                            columns=mattuck4) # set sentneces as the columns\n",
    "matt_sim.insert(0, 'mean_similarity', matt_sim.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>mean_similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>what  want program look like kind smart answer question behavior</th>\n",
       "      <td>0.046144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so way little warm exercise  like look integration problem</th>\n",
       "      <td>0.042752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>now look connection little detail little piece right sort look like</th>\n",
       "      <td>0.042347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so get example search table  like write little flow chart search work</th>\n",
       "      <td>0.041183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so end hour shall able write program like know answer question behavior</th>\n",
       "      <td>0.040889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and  write rule like freeze little plastic bag</th>\n",
       "      <td>0.040420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know people come  major computer science  like write program</th>\n",
       "      <td>0.040179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so get branching type structure look maybe little bit like</th>\n",
       "      <td>0.039813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but know search let speed thing little bit couple search name</th>\n",
       "      <td>0.039805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>there lot thing think like integral e x e x.</th>\n",
       "      <td>0.039516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and go reach way time look program day discuss talk miniature artificial intelligence course</th>\n",
       "      <td>0.039423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and  want talk little bit search model go head</th>\n",
       "      <td>0.039392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and way slagle design program find problem work transformation go loop</th>\n",
       "      <td>0.039083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so go today tomorrow look stuff ask work work need kind question emerge</th>\n",
       "      <td>0.038659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so  go deal world kind unpronounceable term like bacterioide anaerobic stuff like</th>\n",
       "      <td>0.038525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "text                                                mean_similarity\n",
       "text                                                               \n",
       "what  want program look like kind smart answer ...         0.046144\n",
       "so way little warm exercise  like look integrat...         0.042752\n",
       "now look connection little detail little piece ...         0.042347\n",
       "so get example search table  like write little ...         0.041183\n",
       "so end hour shall able write program like know ...         0.040889\n",
       "and  write rule like freeze little plastic bag             0.040420\n",
       " know people come  major computer science  like...         0.040179\n",
       "so get branching type structure look maybe litt...         0.039813\n",
       "but know search let speed thing little bit coup...         0.039805\n",
       "there lot thing think like integral e x e x.               0.039516\n",
       "and go reach way time look program day discuss ...         0.039423\n",
       "and  want talk little bit search model go head             0.039392\n",
       "and way slagle design program find problem work...         0.039083\n",
       "so go today tomorrow look stuff ask work work n...         0.038659\n",
       "so  go deal world kind unpronounceable term lik...         0.038525"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_sim[['mean_similarity']].sort_values(by='mean_similarity',ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>mean_similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>if look word count want include term computer want use threshold near neighbor</th>\n",
       "      <td>0.085655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and answer question like course want people custodian kind knowledge interested</th>\n",
       "      <td>0.078267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>because kind thing computer 's process information like bulldozer process gravel</th>\n",
       "      <td>0.076388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so want guy like  better rid dotted x</th>\n",
       "      <td>0.075556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and  want idea reappear disguised form area expect</th>\n",
       "      <td>0.075458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and want try simple thing try complex likely understand</th>\n",
       "      <td>0.071154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so lecture learning  want spend minute beginning talk lay land</th>\n",
       "      <td>0.070713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very simple calculation head like add number spelling word thing like</th>\n",
       "      <td>0.070142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when think pattern recognition near neighbor base learning get sort mechanism generate vector feature</th>\n",
       "      <td>0.068879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how computer program reach drink cup coffee want cup coffee</th>\n",
       "      <td>0.067249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well genius human human computer figure guy maximum area maximum hole area</th>\n",
       "      <td>0.067211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so  go guess hold knife throat corner total area go like orange cover total area</th>\n",
       "      <td>0.066565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>because  know think mit student pretty tough need sleep stuff</th>\n",
       "      <td>0.065687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and maybe like talk hire computer expert track result</th>\n",
       "      <td>0.065447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and thing talk connection regularity base learning today 's topic near neighbor</th>\n",
       "      <td>0.065135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "text                                                mean_similarity\n",
       "text                                                               \n",
       "if look word count want include term computer w...         0.085655\n",
       "and answer question like course want people cus...         0.078267\n",
       "because kind thing computer 's process informat...         0.076388\n",
       "so want guy like  better rid dotted x                      0.075556\n",
       "and  want idea reappear disguised form area expect         0.075458\n",
       "and want try simple thing try complex likely un...         0.071154\n",
       "so lecture learning  want spend minute beginnin...         0.070713\n",
       "very simple calculation head like add number sp...         0.070142\n",
       "when think pattern recognition near neighbor ba...         0.068879\n",
       "how computer program reach drink cup coffee wan...         0.067249\n",
       "well genius human human computer figure guy max...         0.067211\n",
       "so  go guess hold knife throat corner total are...         0.066565\n",
       "because  know think mit student pretty tough ne...         0.065687\n",
       "and maybe like talk hire computer expert track ...         0.065447\n",
       "and thing talk connection regularity base learn...         0.065135"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_sim[['mean_similarity']].sort_values(by='mean_similarity',ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>mean_similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>so equation literature write way k time y prime plus y equal</th>\n",
       "      <td>0.077077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and mean equation form r square plus time r plus squared equal zero</th>\n",
       "      <td>0.074696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the equation z prime x plus z equal z plus minus z.</th>\n",
       "      <td>0.069086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so  go equation d square plus omega zero squared apply y equal cosine omega</th>\n",
       "      <td>0.064147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so omega equal omega nought equation look like d square plus omega nought squared natural frequency</th>\n",
       "      <td>0.063902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v prime divide minus n equal p x time v plus q x.</th>\n",
       "      <td>0.062898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and thing plus sine omega nought plus omega time</th>\n",
       "      <td>0.062720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time omega k divide product complex conjugate real number plus omega k square</th>\n",
       "      <td>0.062347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so  standard form go look like dtdtd little t plus kt equal k time te</th>\n",
       "      <td>0.062250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and know solution equation look like linear equation fact piece function t.</th>\n",
       "      <td>0.062224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be plus cosine x y prime plus minus sine x derivative time</th>\n",
       "      <td>0.061294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so complex equation go d square plus omega nought squared</th>\n",
       "      <td>0.060432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so look like cosine omega t plus sine omega t.</th>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and  turn stroke equation z prime x plus z equal f z.</th>\n",
       "      <td>0.058505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the equation  go look look like y equal q t let okay constant constant positive</th>\n",
       "      <td>0.057818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "text                                                mean_similarity\n",
       "text                                                               \n",
       "so equation literature write way k time y prime...         0.077077\n",
       "and mean equation form r square plus time r plu...         0.074696\n",
       "the equation z prime x plus z equal z plus minu...         0.069086\n",
       "so  go equation d square plus omega zero square...         0.064147\n",
       "so omega equal omega nought equation look like ...         0.063902\n",
       "v prime divide minus n equal p x time v plus q x.          0.062898\n",
       "and thing plus sine omega nought plus omega time           0.062720\n",
       "time omega k divide product complex conjugate r...         0.062347\n",
       "so  standard form go look like dtdtd little t p...         0.062250\n",
       "and know solution equation look like linear equ...         0.062224\n",
       " be plus cosine x y prime plus minus sine x der...         0.061294\n",
       "so complex equation go d square plus omega noug...         0.060432\n",
       "so look like cosine omega t plus sine omega t.             0.060100\n",
       "and  turn stroke equation z prime x plus z equa...         0.058505\n",
       "the equation  go look look like y equal q t let...         0.057818"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_sim[['mean_similarity']].sort_values(by='mean_similarity',ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>mean_similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>so linear time sort log n log log n item</th>\n",
       "      <td>0.076976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but w 2 log n radix 1.1 time log</th>\n",
       "      <td>0.075710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then 2 time log k 2 log k 2</th>\n",
       "      <td>0.075710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if log n log log n item k item sort linear time</th>\n",
       "      <td>0.074379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and w log 2 plus epsilon time log log n.</th>\n",
       "      <td>0.070839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>because k time log d minus 1 k pre processing rebuild node</th>\n",
       "      <td>0.068915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tree  want identify maximally deep node log n node</th>\n",
       "      <td>0.068418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obviously thing spend log n time search log n array correspond log n subtree</th>\n",
       "      <td>0.067266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so h height tree go like log n minus log log n.</th>\n",
       "      <td>0.066998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>let  k item k log n log log n.</th>\n",
       "      <td>0.066990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so structure size k go k time log d minus 1 k.</th>\n",
       "      <td>0.064825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then like say log square w equal log n log w root log n.</th>\n",
       "      <td>0.064709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but linear time sort pack sort fit log log log n thing word</th>\n",
       "      <td>0.064523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right way log log u query order u space</th>\n",
       "      <td>0.063740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit log n log log n item word</th>\n",
       "      <td>0.062589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "text                                                mean_similarity\n",
       "text                                                               \n",
       "so linear time sort log n log log n item                   0.076976\n",
       "but w 2 log n radix 1.1 time log                           0.075710\n",
       "then 2 time log k 2 log k 2                                0.075710\n",
       "if log n log log n item k item sort linear time            0.074379\n",
       "and w log 2 plus epsilon time log log n.                   0.070839\n",
       "because k time log d minus 1 k pre processing r...         0.068915\n",
       " tree  want identify maximally deep node log n ...         0.068418\n",
       "obviously thing spend log n time search log n a...         0.067266\n",
       "so h height tree go like log n minus log log n.            0.066998\n",
       "let  k item k log n log log n.                             0.066990\n",
       "so structure size k go k time log d minus 1 k.             0.064825\n",
       "then like say log square w equal log n log w ro...         0.064709\n",
       "but linear time sort pack sort fit log log log ...         0.064523\n",
       "right way log log u query order u space                    0.063740\n",
       " fit log n log log n item word                             0.062589"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sim[['mean_similarity']].sort_values(by='mean_similarity',ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>mean_similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>there be operation binary search tree order h time constant time</th>\n",
       "      <td>0.055623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and way sort thing like scheduling sort kind tree include binary tree</th>\n",
       "      <td>0.053985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and time want know sorted order element store nice binary search tree</th>\n",
       "      <td>0.052185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so binary search tree like want know sorted order call order traversal</th>\n",
       "      <td>0.050348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want dive right tell interesting thing like algorithm complexity algorithm</th>\n",
       "      <td>0.050184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that min heap look insert algorithm look require order n time</th>\n",
       "      <td>0.049330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have show binary search tree avl tree order lg n time regular notion time</th>\n",
       "      <td>0.048325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will look different sort algorithm time heap different data structure</th>\n",
       "      <td>0.048191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>any time array go look heap representation array picture right tell heap look like</th>\n",
       "      <td>0.046549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then sort thing like insert extract max heap sort forth</th>\n",
       "      <td>0.046405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and use heap build sort algorithm call heap sort different insertion sort merge sort</th>\n",
       "      <td>0.046309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but time 2 time 12288 minus 5 time 12288 square divide 65536</th>\n",
       "      <td>0.045954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n work n item go constant time order divide array</th>\n",
       "      <td>0.045792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and happy binary insertion sort case number binary insertion sort theta n squared complexity look swap</th>\n",
       "      <td>0.045751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and implement naive algorithm look like tn equal 4 tn 2 plus theta</th>\n",
       "      <td>0.045626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "text                                                mean_similarity\n",
       "text                                                               \n",
       "there be operation binary search tree order h t...         0.055623\n",
       "and way sort thing like scheduling sort kind tr...         0.053985\n",
       "and time want know sorted order element store n...         0.052185\n",
       "so binary search tree like want know sorted ord...         0.050348\n",
       " want dive right tell interesting thing like al...         0.050184\n",
       "that min heap look insert algorithm look requir...         0.049330\n",
       " have show binary search tree avl tree order lg...         0.048325\n",
       " will look different sort algorithm time heap d...         0.048191\n",
       "any time array go look heap representation arra...         0.046549\n",
       "then sort thing like insert extract max heap so...         0.046405\n",
       "and use heap build sort algorithm call heap sor...         0.046309\n",
       "but time 2 time 12288 minus 5 time 12288 square...         0.045954\n",
       "n work n item go constant time order divide array          0.045792\n",
       "and happy binary insertion sort case number bin...         0.045751\n",
       "and implement naive algorithm look like tn equa...         0.045626"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algos_sim[['mean_similarity']].sort_values(by='mean_similarity',ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>mean_similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the equation z prime x plus z equal z plus minus z.</th>\n",
       "      <td>0.155294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v prime divide minus n equal p x time v plus q x.</th>\n",
       "      <td>0.151131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so suppose equation let y prime equal y x minus y square</th>\n",
       "      <td>0.137181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so y prime y square equal x time y minus</th>\n",
       "      <td>0.137071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and  turn stroke equation z prime x plus z equal f z.</th>\n",
       "      <td>0.130385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x oh  like combination plus divide minus y x time</th>\n",
       "      <td>0.118255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so x v prime plus v equal x okay</th>\n",
       "      <td>0.117641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but ultimately way equation solve change linear equation equation variable separable</th>\n",
       "      <td>0.115194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so  answer y equal 2x divide x square plus arbitrary constant</th>\n",
       "      <td>0.113632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a bernoulli equation divide way turn equation minus n sorry</th>\n",
       "      <td>0.113608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for example suppose y prime let x square y divide x cub plus y cub</th>\n",
       "      <td>0.112609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>course conceal write xy prime plus xy prime minus</th>\n",
       "      <td>0.111459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so minus v prime equal x stay x y.</th>\n",
       "      <td>0.110960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so apply equation form y prime equal kind term right hand</th>\n",
       "      <td>0.107686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y prime z prime x plus z time derivative factor</th>\n",
       "      <td>0.107624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "text                                                mean_similarity\n",
       "text                                                               \n",
       "the equation z prime x plus z equal z plus minu...         0.155294\n",
       "v prime divide minus n equal p x time v plus q x.          0.151131\n",
       "so suppose equation let y prime equal y x minus...         0.137181\n",
       "so y prime y square equal x time y minus                   0.137071\n",
       "and  turn stroke equation z prime x plus z equa...         0.130385\n",
       "x oh  like combination plus divide minus y x time          0.118255\n",
       "so x v prime plus v equal x okay                           0.117641\n",
       "but ultimately way equation solve change linear...         0.115194\n",
       "so  answer y equal 2x divide x square plus arbi...         0.113632\n",
       "a bernoulli equation divide way turn equation m...         0.113608\n",
       "for example suppose y prime let x square y divi...         0.112609\n",
       " course conceal write xy prime plus xy prime minus         0.111459\n",
       "so minus v prime equal x stay x y.                         0.110960\n",
       "so apply equation form y prime equal kind term ...         0.107686\n",
       "y prime z prime x plus z time derivative factor            0.107624"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matt_sim[['mean_similarity']].sort_values(by='mean_similarity',ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Extraction Using NMF and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 20\n",
    "#Instantiate nmf instance and new tfidf vectorizer\n",
    "nmf_m = NMF(alpha=0.1, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=400, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          n_components=10 ,\n",
    "          random_state=43, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0) \n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, # drop words that occur in more 50% of the sentences\n",
    "                             min_df=4, # only use words that appear at least 15\n",
    "                             stop_words='english', \n",
    "                             lowercase=True,\n",
    "                             use_idf=True,\n",
    "                             norm=u'l2',  \n",
    "                             smooth_idf=True \n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nmf (strings):\n",
    "    tfidf = vectorizer.fit_transform(strings)\n",
    "    nmf = nmf_m.fit(tfidf)\n",
    "    tfidf_feature_names = vectorizer.get_feature_names()\n",
    "    print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Negative Matrix Factorization solver: Coordinate Descent\n",
      "\n",
      "\n",
      "\n",
      "NMF Topics from Mattuck4\n",
      "\n",
      "\n",
      "Topic #0: \n",
      "prime minus plus equal divide write suppose course substitute turn dx equation tangent time like differentiate squared let square right\n",
      "\n",
      "Topic #1: \n",
      "equation solve bernoulli linear kind method homogeneous problem way let differential set turn reason divide mean little answer word like\n",
      "\n",
      "Topic #2: \n",
      "square root log plus squared divide turn suppose equal right let answer time constant tangent hope minus function good equation\n",
      "\n",
      "Topic #3: \n",
      "substitution use direct write inverse equal course problem differentiate instead able try set new variable solve like way look make\n",
      "\n",
      "Topic #4: \n",
      "change variable mean make x1 course new substitute hope answer inverse way divide look unit solve equation let plus substitution\n",
      "\n",
      "Topic #5: \n",
      "angle boat beam 45 slope light function degree tangent word make little want reason let plus like think differential try\n",
      "\n",
      "Topic #6: \n",
      "constant temperature think answer form make reason dx divide look write equal way slope x1 plus word new mean function\n",
      "\n",
      "Topic #7: \n",
      "hand right left word variable able multiply x1 dx minus linear squared form write new term function kind hope homogeneous\n",
      "\n",
      "Topic #8: \n",
      "good look little pretty let way combine term like think multiply homogeneous turn divide want hope light try form write\n",
      "\n",
      "Topic #9: \n",
      "time unit let inverse plus divide use instead tangent differentiate like degree mean variable new x1 log 45 able root\n",
      "\n",
      "NMF Topics from Winston AI 10\n",
      "\n",
      "\n",
      "Topic #0: \n",
      "want shall record memory know variance need computer acceleration trajectory line guy ball idea bisector perpendicular look piece use let\n",
      "\n",
      "Topic #1: \n",
      "area total cover hole guy come like guess idea include computer boundary look little easy line think measure just want\n",
      "\n",
      "Topic #2: \n",
      "sleep hour day need stuff 10 way guess know say come time interested memory record think line value desire good\n",
      "\n",
      "Topic #3: \n",
      "talk learning near neighbor sort today stuff recognition thing idea think particular vector computer want ball distance look use let\n",
      "\n",
      "Topic #4: \n",
      "theta problem time desire let ball think angle space acceleration trajectory easy value interested equal say way good particular 10\n",
      "\n",
      "Topic #5: \n",
      "thing like kind likely simple just computer cover let interested distance want ball space guy near boundary easy question talk\n",
      "\n",
      "Topic #6: \n",
      "learn good idea kind easy time particular article learning stuff recognition talk value use space equal interested acceleration look variance\n",
      "\n",
      "Topic #7: \n",
      "people think country town question little easy piece article boundary kind divide interested sort bisector perpendicular particular like shall recognition\n",
      "\n",
      "Topic #8: \n",
      "word include computer article use question idea look like neighbor equal value simple near want space time hole good particular\n",
      "\n",
      "Topic #9: \n",
      "vector measure distance angle come divide let include line look near article bisector perpendicular thing recognition sort space area neighbor\n",
      "\n",
      "\n",
      "NMF Topics from Differential Equations\n",
      "\n",
      "\n",
      "Topic #0: \n",
      "prime minus equal plus double write divide zero let x2 okay xy differentiate stay substitute term suppose alpha form dx\n",
      "\n",
      "Topic #1: \n",
      "equation solve differential solution way linear order form homogeneous write talk know variable bernoulli particular standard lecture general term change\n",
      "\n",
      "Topic #2: \n",
      "omega nought zero cosine squared frequency divide minus sine thing amplitude input approach oscillation allow natural value pi spring add\n",
      "\n",
      "Topic #3: \n",
      "slope line method curve euler point good element want thing little draw word let integral computer mean value direction right\n",
      "\n",
      "Topic #4: \n",
      "number complex real problem use function value turn course good domain variable exponential polar law answer set let lot zero\n",
      "\n",
      "Topic #5: \n",
      "theta cosine sine hand plus right vector form real left negative write derivative factor angle phi product want exponential respect\n",
      "\n",
      "Topic #6: \n",
      "time function kt plus negative equal power multiply concentration little unit sorry integral derivative minute mean rt vary talk term\n",
      "\n",
      "Topic #7: \n",
      "like look thing form solution way frequency variable step example general input standard amplitude okay size neutral natural know big\n",
      "\n",
      "Topic #8: \n",
      "constant arbitrary solution word equal spring answer positive coefficient function kt general fact book lump k1 times set decay certain\n",
      "\n",
      "Topic #9: \n",
      "square plus root squared log leave coefficient negative polynomial quantity equation characteristic inside ib turn real maybe equal nought bi\n",
      "\n",
      "\n",
      "NMF Topics from AI\n",
      "\n",
      "\n",
      "Topic #0: \n",
      "tree question answer goal behavior program know build kind forward ask way work backward probability shall leave 80 base particular\n",
      "\n",
      "Topic #1: \n",
      "minus plus alpha equal square sub negative sum integral sample fourth gutter function dx time close oh different situation likewise\n",
      "\n",
      "Topic #2: \n",
      "little bit talk want course day subject tell start shall solution learn intelligence model type maybe example artificial turn look\n",
      "\n",
      "Topic #3: \n",
      "search path depth want use breadth queue extend goal node good beam instead quiz order close heuristic pretty british museum\n",
      "\n",
      "Topic #4: \n",
      "problem solve kind need work transformation way order talk final method idea table test slagle algorithm program think today huffman\n",
      "\n",
      "Topic #5: \n",
      "line junction right boundary way label object arrow draw constraint arrange possibility street form face fork discover try possible world\n",
      "\n",
      "Topic #6: \n",
      "vector sample partial dot function respect product time street decision p2 depend great discover want performance value derivative width magnitude\n",
      "\n",
      "Topic #7: \n",
      "write neural net program world year time paper intelligence model people computer simple train artificial actually age day able deal\n",
      "\n",
      "Topic #8: \n",
      "like look rule think know say people expert neuron professor instead base example function ought come maybe piece deep imagine\n",
      "\n",
      "Topic #9: \n",
      "thing stuff work say simple lot level knowledge transformation safe list suggest story node rule idea number far situation plug\n",
      "\n",
      "\n",
      "NMF Topics from Data Structures\n",
      "\n",
      "\n",
      "Topic #0: \n",
      "log divide epsilon square sort query word size factor subtree base space bind afford time achieve overall fit update pay\n",
      "\n",
      "Topic #1: \n",
      "time constant number access touch key build linear compute operation know algorithm update trie need sequence travel change order cost\n",
      "\n",
      "Topic #2: \n",
      "tree search binary know way model fast build good problem optimal balanced basically actually obvious nice start question turn black\n",
      "\n",
      "Topic #3: \n",
      "node pointer store touch array want root subtree visit path version new tree ancestor particular let old know leaf rotation\n",
      "\n",
      "Topic #4: \n",
      "emde van boas thing size sort root word use think algorithm author way kind number ram happen square basically cache\n",
      "\n",
      "Topic #5: \n",
      "item insert want right word interval array list order delete guy store size promote buffer sort small cluster half shift\n",
      "\n",
      "Topic #6: \n",
      "like point look set add level rectangle picture kind right mean guess want past thing path guy ok access equal\n",
      "\n",
      "Topic #7: \n",
      "space bit little big query need store order linear label table want assume use achieve array super square small oh\n",
      "\n",
      "Topic #8: \n",
      "plus epsilon minus divide mean use half order rmq basically recursion time decrease mod split value problem assume large general\n",
      "\n",
      "Topic #9: \n",
      "structure datum data version pointer machine need new model actually cache update different old late predecessor want general technique say\n",
      "\n",
      "\n",
      "NMF Topics from Algorithms\n",
      "\n",
      "\n",
      "Topic #0: \n",
      "tree binary search lg height insert structure balance actually avl delete need leaf thing lecture happen check sorted let good\n",
      "\n",
      "Topic #1: \n",
      "heap max build heapify property min run invariant structure array node maintain child trivial extract root unordered different violate big\n",
      "\n",
      "Topic #2: \n",
      "minus divide plus square equal xi sub root epsilon compute great like raise lg newton let method comma function xn\n",
      "\n",
      "Topic #3: \n",
      "algorithm complexity problem shall class python talk different set good efficient version correspond comparison input peak write correct term analyze\n",
      "\n",
      "Topic #4: \n",
      "time constant order word item operation case lg linear spend array work run bad sum et cetera landing think bunch\n",
      "\n",
      "Topic #5: \n",
      "sort insertion merge like way array use look thing count place theta example run auxiliary turn structure space kind particular\n",
      "\n",
      "Topic #6: \n",
      "log theta base raise write step swap alpha way equal compare complexity bound squared mean insertion end cost time prove\n",
      "\n",
      "Topic #7: \n",
      "look right element left child key order way value list slot peak let index case great subtree array need add\n",
      "\n",
      "Topic #8: \n",
      "want thing know large like able item start kind size insert sense set maybe important compute respect multiply nice minute\n",
      "\n",
      "Topic #9: \n",
      "number node level add term work digit little small constant function hash precision write associate factor leaf bit start chain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Non Negative Matrix Factorization solver: Coordinate Descent\\n\\n')\n",
    "print('\\nNMF Topics from Mattuck4\\n')\n",
    "print_nmf(mattuck4)\n",
    "print('NMF Topics from Winston AI 10\\n')\n",
    "print_nmf(winstonai10)\n",
    "print('\\nNMF Topics from Differential Equations\\n')\n",
    "print_nmf(diff_eq)\n",
    "print('\\nNMF Topics from AI\\n')\n",
    "print_nmf(AI)\n",
    "print('\\nNMF Topics from Data Structures\\n')\n",
    "print_nmf(datas)\n",
    "print('\\nNMF Topics from Algorithms\\n')\n",
    "print_nmf(algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use word frequencies with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 500\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50,\n",
    "                                random_state=0)\n",
    "\n",
    "cvectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lda (string):\n",
    "    tf = cvectorizer.fit_transform(string)\n",
    "    lda.fit(tf)\n",
    "    print(\"\\nTopics in LDA model:\\n\")\n",
    "    tf_feature_names = cvectorizer.get_feature_names()\n",
    "    print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA Topics in Winston AI 10\n",
      "\n",
      "\n",
      "Topics in LDA model:\n",
      "\n",
      "\n",
      "Topic #0: \n",
      "talk stuff thing learning near like pattern neighbor word recognition computer straight today base magazine lot town country trajectory position\n",
      "\n",
      "Topic #1: \n",
      "say feature vector come day way worth invent compare recognition happen easy good library value guy decision control world robot\n",
      "\n",
      "Topic #2: \n",
      "area total cover want like concert hole electrical guy custodian attempt come sort include shall measure maximum idea knowledge let\n",
      "\n",
      "Topic #3: \n",
      "stuff velocity acceleration guy want know ball think sleep speed particular trajectory need associate movement value position look talk arm\n",
      "\n",
      "Topic #4: \n",
      "little good variance particular piece movement try 100 record associate want shall just stuff think table easy prime right time\n",
      "\n",
      "Topic #5: \n",
      "perpendicular divide bisector human space article area maximum simple instead use construct boundary line decision thing talk computer want equal\n",
      "\n",
      "Topic #6: \n",
      "10 pitch sleep need want memory try 25 know day original 20 record hour likely run simple time worth guess\n",
      "\n",
      "Topic #7: \n",
      "word similar respect count likely learn hack question table idea hacking maybe include run probe library article compare custodian line\n",
      "\n",
      "Topic #8: \n",
      "problem angle theta kind learn want vector let measure hour sleep music easy think include distance people like answer need\n",
      "\n",
      "Topic #9: \n",
      "value theta time dog equal fat cat sum think drink talk art question desire variance original interested hacking computer sort\n",
      "\n",
      "\n",
      "LDA Topics in Differential Equations \n",
      "\n",
      "\n",
      "Topics in LDA model:\n",
      "\n",
      "\n",
      "Topic #0: \n",
      "value plus negative minus form want equal answer prime way zero write real general number positive time standard law course\n",
      "\n",
      "Topic #1: \n",
      "point y1 calculate mean word factor curve minute zero omega angle rt slope cosine integral theorem times sort y2 room\n",
      "\n",
      "Topic #2: \n",
      "number theta complex exponential cosine unit vector angle know sine high word involve case product formula hand euler expression law\n",
      "\n",
      "Topic #3: \n",
      "solution curve initial word constant equation condition start form half equal salt steady concentration long particular differential geometric state term\n",
      "\n",
      "Topic #4: \n",
      "let little constant temperature good euler method example concentration want use formula bit work model step equation external size try\n",
      "\n",
      "Topic #5: \n",
      "function want equal word number way talk spring method use think frequency respect slope mass good draw like complex okay\n",
      "\n",
      "Topic #6: \n",
      "equation hand solve differential linear right solution left term homogeneous kind talk form prime like method function time bernoulli think\n",
      "\n",
      "Topic #7: \n",
      "plus like omega time look equation square problem cosine equal way minus constant variable squared use thing root real sine\n",
      "\n",
      "Topic #8: \n",
      "think rate hard step flow decay thing concentration change ask size angle mean factor quickly question multiply certain amplitude integrating\n",
      "\n",
      "Topic #9: \n",
      "time thing omega slope line little derivative good draw zero point indicate element like curve mean way term infinity phi\n",
      "\n",
      "\n",
      "LDA Topics in AI \n",
      "\n",
      "\n",
      "Topics in LDA model:\n",
      "\n",
      "\n",
      "Topic #0: \n",
      "line try output street decide boundary object kind way junction maximize constraint possible produce label link desire draw content idea\n",
      "\n",
      "Topic #1: \n",
      "tree path know queue thinking thing subject model idea support imagine neuron goal extend theory like level circuit early intelligence\n",
      "\n",
      "Topic #2: \n",
      "come like vector label junction eliminate guy space suggest tell line word face way want vertex transformation write dot maybe\n",
      "\n",
      "Topic #3: \n",
      "minus plus partial equal alpha number respect time sub square change sample p2 derivative integral dx negative vertex function fourth\n",
      "\n",
      "Topic #4: \n",
      "like thing rule work simple problem stuff function look need search say kind think base order situation instead transformation deal\n",
      "\n",
      "Topic #5: \n",
      "complexity program use algorithm knowledge good disease help museum british search say waltz huffman set behavior write beam mycin piece\n",
      "\n",
      "Topic #6: \n",
      "little bit idea talk node start path involve junction object possible knowledge type day level extend arrow belong want sort\n",
      "\n",
      "Topic #7: \n",
      "right look want line stuff boundary object model arrow piece way like actually case let program walk octant hand search\n",
      "\n",
      "Topic #8: \n",
      "question answer goal behavior tree close thing know program number search heuristic kind ask perceptual build net neural write park\n",
      "\n",
      "Topic #9: \n",
      "problem way write time kind program solve junction little work like lot talk sample year course think huffman look world\n",
      "\n",
      "\n",
      "LDA Topics in Data Structures \n",
      "\n",
      "\n",
      "Topics in LDA model:\n",
      "\n",
      "\n",
      "Topic #0: \n",
      "node point touch level ancestor set child time equal property add rectangle order look height visit mean step heap threshold\n",
      "\n",
      "Topic #1: \n",
      "item insert want little big right bit guy store array small half node delete interval tree root maybe like let\n",
      "\n",
      "Topic #2: \n",
      "log time space plus sort constant query square factor linear epsilon order access item number divide size base word key\n",
      "\n",
      "Topic #3: \n",
      "optimal access buffer dynamic splay original prove value sequence string upper second finger measure bind property start stay dynamically sequential\n",
      "\n",
      "Topic #4: \n",
      "tree search structure binary know problem different actually datum way solve data need thing look good think like kind time\n",
      "\n",
      "Topic #5: \n",
      "min table group lookup change type hash just think state field actual cluster store need basically mean apply version check\n",
      "\n",
      "Topic #6: \n",
      "cache general oblivious look fit memory block lecture number level bad operation assumption word need model tall read point version\n",
      "\n",
      "Topic #7: \n",
      "pointer like list node label thing store constant path machine word item number want look know time size leaf guess\n",
      "\n",
      "Topic #8: \n",
      "thing subtree version size write need constant number update make stuff work minus cost happen element van emde little want\n",
      "\n",
      "Topic #9: \n",
      "assume order shift say ok cascading fractional right version leave reverse structure bit data node plus mean specify late sorry\n",
      "\n",
      "\n",
      "LDA Topics in Algorithms\n",
      "\n",
      "\n",
      "Topics in LDA model:\n",
      "\n",
      "\n",
      "Topic #0: \n",
      "case peak start bad row careful code professor base counting scenario structure 496 try datum little trial second difference multiplication\n",
      "\n",
      "Topic #1: \n",
      "complexity algorithm simple good think basically class method problem shall sense term newton type structure associate real overall datum operation\n",
      "\n",
      "Topic #2: \n",
      "search algorithm binary list tree insert python comparison happen sorted correspond model point write pointer want know like way version\n",
      "\n",
      "Topic #3: \n",
      "log lg theta swap base step max compare time algorithm way build term heap straightforward specific squared analysis complexity word\n",
      "\n",
      "Topic #4: \n",
      "sort heap max want array structure hash talk algorithm different time thing example run assumption use merge good insertion function\n",
      "\n",
      "Topic #5: \n",
      "time square equal constant root divide number plus peak lg analysis compute epsilon case theta sum great algorithm care essentially\n",
      "\n",
      "Topic #6: \n",
      "time want work large constant add number raise able say let long small algorithm little item digit talk level order\n",
      "\n",
      "Topic #7: \n",
      "tree like look thing right minus element way sort order number plus need know xi problem item equal solve set\n",
      "\n",
      "Topic #8: \n",
      "key max node heap delete child subtree slot value left height right look property write element path plus particular insert\n",
      "\n",
      "Topic #9: \n",
      "array hash table possible document chain open word turn little addressing answer prime basically let contain value list function strategy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLDA Topics in Winston AI 10\\n\")\n",
    "print_lda(winstonai10)\n",
    "print(\"\\nLDA Topics in Differential Equations \\n\")\n",
    "print_lda(diff_eq)\n",
    "print(\"\\nLDA Topics in AI \\n\")\n",
    "print_lda(AI)\n",
    "print(\"\\nLDA Topics in Data Structures \\n\")\n",
    "print_lda(datas)\n",
    "print(\"\\nLDA Topics in Algorithms\\n\")\n",
    "print_lda(algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
